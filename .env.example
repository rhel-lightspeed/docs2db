# Docs2DB Configuration
# Copy this file to .env and fill in your values

# ============================================================================
# Content Directory
# ============================================================================
# Base directory for storing content files
# CONTENT_BASE_DIR=content

# ============================================================================
# LLM Provider Settings for Contextual Chunking
# ============================================================================

# Skip LLM contextual chunk generation entirely (faster, lower quality)
# LLM_SKIP_CONTEXT=false

# LLM model to use for contextual chunk generation
# LLM_CONTEXT_MODEL=qwen2.5:7b-instruct

# OpenAI-compatible API URL (for Ollama, OpenAI, etc.)
# Leave commented to use default Ollama endpoint (http://localhost:11434)
# LLM_OPENAI_URL=http://localhost:11434
# LLM_OPENAI_URL=https://api.openai.com

# IBM WatsonX API URL (mutually exclusive with LLM_OPENAI_URL)
# LLM_WATSONX_URL=https://us-south.ml.cloud.ibm.com

# Override model context limit (in tokens) for map-reduce summarization
# Only needed if the built-in limits are incorrect for your model
# LLM_CONTEXT_LIMIT_OVERRIDE=32768

# ============================================================================
# WatsonX Credentials
# ============================================================================
# Only needed if using IBM WatsonX (LLM_WATSONX_URL is set)

# WatsonX API Key (get from IBM Cloud)
# WATSONX_API_KEY=your-api-key-here

# WatsonX Project ID
# WATSONX_PROJECT_ID=your-project-id-here

# ============================================================================
# Common LLM Provider Examples
# ============================================================================

# Example 1: Ollama (default, fastest for local development)
# LLM_CONTEXT_MODEL=qwen2.5:7b-instruct
# LLM_OPENAI_URL=http://localhost:11434

# Example 2: Ollama with faster model
# LLM_CONTEXT_MODEL=qwen2.5:3b-instruct
# LLM_OPENAI_URL=http://localhost:11434

# Example 3: OpenAI
# LLM_CONTEXT_MODEL=gpt-4o-mini
# LLM_OPENAI_URL=https://api.openai.com
# Also set OPENAI_API_KEY environment variable

# Example 4: IBM WatsonX
# LLM_CONTEXT_MODEL=ibm/granite-3-8b-instruct
# LLM_WATSONX_URL=https://us-south.ml.cloud.ibm.com
# WATSONX_API_KEY=your-key
# WATSONX_PROJECT_ID=your-project-id

# Example 5: Skip contextual chunking (fastest)
# LLM_SKIP_CONTEXT=true

# ============================================================================
# Notes
# ============================================================================
# - CLI flags always override environment variables
# - LLM_OPENAI_URL and LLM_WATSONX_URL are mutually exclusive
# - Map-reduce summarization triggers automatically for large documents
# - See docs/LLM_PROVIDERS.md for detailed provider configuration


# ============================================================================
# Chunking Settings
# ============================================================================

# File pattern for finding source documents to chunk
# CHUNKING_PATTERN=**/*.json

# ============================================================================
# Embedding Settings
# ============================================================================

# Embedding model to use for generating vector embeddings
# EMBEDDING_MODEL=granite-30m-english

# File pattern for finding chunks files to embed
# EMBEDDING_PATTERN=**/*.chunks.json
